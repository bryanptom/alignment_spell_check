{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6f0f6953",
   "metadata": {},
   "outputs": [],
   "source": [
    "import layoutparser as lp\n",
    "import json\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "import pytesseract\n",
    "import cv2\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "import os\n",
    "from collections import defaultdict\n",
    "from matplotlib import pyplot as plt\n",
    "\n",
    "pytesseract.pytesseract.tesseract_cmd = r'C:\\Program Files\\Tesseract-OCR-5-0\\tesseract.exe'\n",
    "ocr_agent = lp.TesseractAgent(languages = 'eng')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "5a5eded6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "data folder is set to `C:\\Users\\bryan\\Documents\\NBER\\OCR_error_correction\\code\\../../neuspell\\neuspell\\../data` script\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "sys.path.insert(1, '../../neuspell')\n",
    "import neuspell\n",
    "from neuspell import available_checkers, BertChecker"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "bb03a7da",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "available checkers: ['BertsclstmChecker', 'CnnlstmChecker', 'NestedlstmChecker', 'SclstmChecker', 'SclstmbertChecker', 'BertChecker']\n"
     ]
    }
   ],
   "source": [
    "print(f'available checkers: {neuspell.available_checkers()}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "b34c0ad4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loading vocab from path:C:\\Users\\bryan\\Documents\\NBER\\OCR_error_correction\\code\\../../neuspell\\neuspell\\../data/checkpoints/subwordbert-probwordnoise\\vocab.pkl\n",
      "initializing model\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-cased were not used when initializing BertModel: ['cls.seq_relationship.weight', 'cls.predictions.decoder.weight', 'cls.seq_relationship.bias', 'cls.predictions.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.LayerNorm.weight']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of parameters in the model: 185211810\n",
      "Loading model params from checkpoint dir: C:\\Users\\bryan\\Documents\\NBER\\OCR_error_correction\\code\\../../neuspell\\neuspell\\../data/checkpoints/subwordbert-probwordnoise\n"
     ]
    },
    {
     "ename": "UnpicklingError",
     "evalue": "invalid load key, '<'.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mUnpicklingError\u001b[0m                           Traceback (most recent call last)",
      "Input \u001b[1;32mIn [7]\u001b[0m, in \u001b[0;36m<cell line: 2>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      1\u001b[0m checker \u001b[38;5;241m=\u001b[39m BertChecker()\n\u001b[1;32m----> 2\u001b[0m \u001b[43mchecker\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfrom_pretrained\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\Documents\\NBER\\OCR_error_correction\\code\\../../neuspell\\neuspell\\corrector.py:140\u001b[0m, in \u001b[0;36mCorrector.from_pretrained\u001b[1;34m(self, ckpt_path, vocab_path, **kwargs)\u001b[0m\n\u001b[0;32m    139\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mfrom_pretrained\u001b[39m(\u001b[38;5;28mself\u001b[39m, ckpt_path\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, vocab_path\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m--> 140\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_from_pretrained\u001b[49m\u001b[43m(\u001b[49m\u001b[43mckpt_path\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvocab_path\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\Documents\\NBER\\OCR_error_correction\\code\\../../neuspell\\neuspell\\corrector.py:135\u001b[0m, in \u001b[0;36mCorrector._from_pretrained\u001b[1;34m(self, ckpt_path, vocab_path)\u001b[0m\n\u001b[0;32m    132\u001b[0m     download_pretrained_model(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mckpt_path)\n\u001b[0;32m    134\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mload_output_vocab(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mvocab_path)\n\u001b[1;32m--> 135\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mload_model\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mckpt_path\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    137\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m\n",
      "File \u001b[1;32m~\\Documents\\NBER\\OCR_error_correction\\code\\../../neuspell\\neuspell\\corrector_subwordbert.py:29\u001b[0m, in \u001b[0;36mBertChecker.load_model\u001b[1;34m(self, ckpt_path)\u001b[0m\n\u001b[0;32m     27\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124minitializing model\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m     28\u001b[0m initialized_model \u001b[38;5;241m=\u001b[39m load_model(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mvocab)\n\u001b[1;32m---> 29\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel \u001b[38;5;241m=\u001b[39m \u001b[43mload_pretrained\u001b[49m\u001b[43m(\u001b[49m\u001b[43minitialized_model\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mckpt_path\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\Documents\\NBER\\OCR_error_correction\\code\\../../neuspell\\neuspell\\seq_modeling\\subwordbert.py:32\u001b[0m, in \u001b[0;36mload_pretrained\u001b[1;34m(model, checkpoint_path, optimizer, device)\u001b[0m\n\u001b[0;32m     29\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mLoading model params from checkpoint dir: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mcheckpoint_path\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m     31\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m---> 32\u001b[0m     checkpoint_data \u001b[38;5;241m=\u001b[39m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mload\u001b[49m\u001b[43m(\u001b[49m\u001b[43mos\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpath\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mjoin\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcheckpoint_path\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mpytorch_model.bin\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmap_location\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmap_location\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     33\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mFileNotFoundError\u001b[39;00m:\n\u001b[0;32m     34\u001b[0m     download_pretrained_model(checkpoint_path)\n",
      "File \u001b[1;32m~\\Anaconda3\\lib\\site-packages\\torch\\serialization.py:608\u001b[0m, in \u001b[0;36mload\u001b[1;34m(f, map_location, pickle_module, **pickle_load_args)\u001b[0m\n\u001b[0;32m    606\u001b[0m             \u001b[38;5;28;01mreturn\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mjit\u001b[38;5;241m.\u001b[39mload(opened_file)\n\u001b[0;32m    607\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m _load(opened_zipfile, map_location, pickle_module, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mpickle_load_args)\n\u001b[1;32m--> 608\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_legacy_load\u001b[49m\u001b[43m(\u001b[49m\u001b[43mopened_file\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmap_location\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpickle_module\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mpickle_load_args\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\Anaconda3\\lib\\site-packages\\torch\\serialization.py:777\u001b[0m, in \u001b[0;36m_legacy_load\u001b[1;34m(f, map_location, pickle_module, **pickle_load_args)\u001b[0m\n\u001b[0;32m    771\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(f, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mreadinto\u001b[39m\u001b[38;5;124m'\u001b[39m) \u001b[38;5;129;01mand\u001b[39;00m (\u001b[38;5;241m3\u001b[39m, \u001b[38;5;241m8\u001b[39m, \u001b[38;5;241m0\u001b[39m) \u001b[38;5;241m<\u001b[39m\u001b[38;5;241m=\u001b[39m sys\u001b[38;5;241m.\u001b[39mversion_info \u001b[38;5;241m<\u001b[39m (\u001b[38;5;241m3\u001b[39m, \u001b[38;5;241m8\u001b[39m, \u001b[38;5;241m2\u001b[39m):\n\u001b[0;32m    772\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\n\u001b[0;32m    773\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtorch.load does not work with file-like objects that do not implement readinto on Python 3.8.0 and 3.8.1. \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    774\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mReceived object of type \u001b[39m\u001b[38;5;130;01m\\\"\u001b[39;00m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mtype\u001b[39m(f)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;130;01m\\\"\u001b[39;00m\u001b[38;5;124m. Please update to Python 3.8.2 or newer to restore this \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    775\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfunctionality.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m--> 777\u001b[0m magic_number \u001b[38;5;241m=\u001b[39m \u001b[43mpickle_module\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mload\u001b[49m\u001b[43m(\u001b[49m\u001b[43mf\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mpickle_load_args\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    778\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m magic_number \u001b[38;5;241m!=\u001b[39m MAGIC_NUMBER:\n\u001b[0;32m    779\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mInvalid magic number; corrupt file?\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[1;31mUnpicklingError\u001b[0m: invalid load key, '<'."
     ]
    }
   ],
   "source": [
    "checker = BertChecker()\n",
    "checker.from_pretrained()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "64dbc323",
   "metadata": {},
   "outputs": [],
   "source": [
    "def apply_brightness_contrast(input_img, brightness = 0, contrast = 0):\n",
    "    \n",
    "    if brightness != 0:\n",
    "        if brightness > 0:\n",
    "            shadow = brightness\n",
    "            highlight = 255\n",
    "        else:\n",
    "            shadow = 0\n",
    "            highlight = 255 + brightness\n",
    "        alpha_b = (highlight - shadow)/255\n",
    "        gamma_b = shadow\n",
    "        \n",
    "        buf = cv2.addWeighted(input_img, alpha_b, input_img, 0, gamma_b)\n",
    "    else:\n",
    "        buf = input_img.copy()\n",
    "    \n",
    "    if contrast != 0:\n",
    "        f = 131*(contrast + 127)/(127*(131-contrast))\n",
    "        alpha_c = f\n",
    "        gamma_c = 127*(1-f)\n",
    "        \n",
    "        buf = cv2.addWeighted(buf, alpha_c, buf, 0, gamma_c)\n",
    "\n",
    "    return buf"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0f1d1d2",
   "metadata": {},
   "source": [
    "Load in Data from JSON"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c55c5197",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(r'../data/chronam_gt_coco.json', 'r') as infile:\n",
    "    coco = json.load(infile)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d11351a2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dict_keys(['info', 'licenses', 'images', 'annotations', 'categories'])\n"
     ]
    }
   ],
   "source": [
    "print(coco.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "0a06f7e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Create a dict mapping images to a dict of associated annotations, keyed by anno id\n",
    "image_boxes = defaultdict(dict)\n",
    "for anno in coco['annotations']:\n",
    "    image_boxes[anno['image_id']][anno['id']] = anno"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7652eb16",
   "metadata": {},
   "source": [
    "Function to apply tesseract to each of the images, getting text from each annotated bounding box"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b2a09fee",
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse_newspaper_boxes(image_path, annotations):\n",
    "    \"\"\"\n",
    "    Extract the text within each annotation section of a newspaper scan \n",
    "    \n",
    "    input: image_path: path to image\n",
    "            annotations: list of annotation objects to transcribe text from\n",
    "            \n",
    "    return: \n",
    "    \"\"\"\n",
    "    image = cv2.imread(f'../data/images/{image_path}')\n",
    "    image = apply_brightness_contrast(image, contrast = 30)\n",
    "    \n",
    "    res = ocr_agent.detect(image, return_response=True)\n",
    "    layout = ocr_agent.gather_data(res, agg_level=lp.TesseractFeatureType.WORD)\n",
    "    \n",
    "    ano_texts = []\n",
    "    for i, ano in enumerate(annotations):\n",
    "        \n",
    "        x0, y0, w, h = ano['bbox']\n",
    "        box_text = layout.filter_by(\n",
    "            lp.Rectangle(x_1=x0, y_1=y0, x_2=x0 + w, y_2=y0 + h)\n",
    "        ).get_texts()\n",
    "        \n",
    "        if not box_text:\n",
    "            box_text = ''\n",
    "        else:\n",
    "            box_text = ' '.join(box_text)\n",
    "        \n",
    "        ano_texts.append({'ano_id': ano['id'],\n",
    "                          'text': box_text})        \n",
    "    return {ano['ano_id']: ano['text'] for ano in ano_texts}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "dd24b040",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████████████████████████████████████████| 10/10 [24:15<00:00, 145.58s/it]\n"
     ]
    }
   ],
   "source": [
    "image_ids = []\n",
    "image_ocr_results = {}\n",
    "for image in tqdm(coco['images']):\n",
    "    image_ids.append(image['id'])\n",
    "    image_ocr_results[image['id']] = parse_newspaper_boxes(image['file_name'], image_boxes[image['id']].values())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5f1ae86",
   "metadata": {},
   "source": [
    "Save the transcribed results to file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "5b8188d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(r'../data/image_ocr_results.json', 'w', encoding = 'utf8') as outfile:\n",
    "    json.dump(image_ocr_results, outfile, indent = 4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "df78de14",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  ee MAINE, THU    EVERY MORNING (SUNDAYS EXCEPTED), BY SPRAGUE & SUN, TERMS: SEVEN DOLLARS PER ANNUM. SINGLE COPIES, TH   —--   MORNING, JULY 1, 1880. REE CENTS.\n"
     ]
    }
   ],
   "source": [
    "print(image_ocr_results[0][107])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f38d3168",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
